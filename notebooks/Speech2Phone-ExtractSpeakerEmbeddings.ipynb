{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a noteboook used to generate the speaker embeddings with the Speech2Phone model for multi-speaker training.\n",
    "\n",
    "Before running this script please DON'T FORGET: \n",
    "- to set file paths.\n",
    "- to download related model files from TTS.\n",
    "- download or clone related repos, linked below.\n",
    "- setup the repositories. ```python setup.py install```\n",
    "- to checkout right commit versions (given next to the model) of TTS.\n",
    "- to set the right paths in the cell below.\n",
    "\n",
    "Repositories:\n",
    "- TTS: https://github.com/mozilla/TTS\n",
    "- Speech2Phone: https://github.com/Edresson/Speech2Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import importlib\n",
    "import random\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from TTS.utils.generic_utils import load_config\n",
    "from tqdm import tqdm\n",
    "from TTS.utils.speakers import save_speaker_mapping, load_speaker_mapping\n",
    "\n",
    "# you may need to change this depending on your system\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "ROOT_PATH = '../../'\n",
    "CONFIG_PATH = os.path.join(ROOT_PATH, 'config.json')\n",
    "CONFIG = load_config(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Speech2Phone Requeriments\n",
    "!pip install pydub tensorflow==1.14.0 tflearn==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Speech2Phone Checkpoint\n",
    "!wget -O ./saver.zip https://www.dropbox.com/s/b19xt2wu3th9p36/Save-Models-Speaker-Diarization.zip?dl=0\n",
    "!mkdir Speech2Phone\n",
    "!unzip saver.zip\n",
    "!mv  Save-Models/  Speech2Phone/Save-Models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils for Speech2Phone Preprocessing\n",
    "from pydub import AudioSegment as audio\n",
    "\n",
    "def detect_leading_silence(sound, silence_threshold=-50.0, chunk_size=10):\n",
    "    '''\n",
    "    sound is a pydub.AudioSegment\n",
    "    silence_threshold in dB\n",
    "    chunk_size in ms\n",
    " \n",
    "    iterate over chunks until you find the first one with sound\n",
    "    '''\n",
    "    trim_ms = 0  # ms\n",
    "    while sound[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold:\n",
    "        #print(trim_ms,len(sound))\n",
    "        if trim_ms > len(sound):\n",
    "            return None\n",
    "        trim_ms += chunk_size\n",
    " \n",
    "    return trim_ms\n",
    "\n",
    "def remove_silence(sound):\n",
    "    start_trim = detect_leading_silence(sound)\n",
    "    if start_trim is None:\n",
    "        return None\n",
    "    end_trim = detect_leading_silence(sound.reverse())\n",
    "    duration = len(sound)\n",
    "    trimmed_sound = sound[start_trim:duration-end_trim]\n",
    "    return trimmed_sound\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "\n",
    "#Create model for restore\n",
    "encoder = tflearn.input_data(shape=[None, 13,int(216)])\n",
    "encoder = tflearn.dropout(encoder,0.9) #10 % drop - 90% -> 80\n",
    "encoder = tflearn.dropout(encoder,0.2)# 80 % drop\n",
    "encoder = tflearn.fully_connected(encoder, 40,activation='crelu')\n",
    "decoder = tflearn.fully_connected(encoder, int(572), activation='linear')\n",
    "net = tflearn.regression(decoder, optimizer='adam', learning_rate=0.0007,loss='mean_square', metric=None)#categorical_crossentropy\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0,tensorboard_dir='tflearn_logs')\n",
    "\n",
    "model.load('./Speech2Phone/Save-Models/Model3-Best-40loc.tflearn')\n",
    "\n",
    "encoding_model = tflearn.DNN(encoder, session=model.session)# used for extract embedding in encoder layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess dataset\n",
    "meta_data = []\n",
    "datasets=CONFIG.datasets\n",
    "for dataset in datasets:\n",
    "    preprocessor = importlib.import_module('TTS.datasets.preprocess')\n",
    "    preprocessor = getattr(preprocessor,  dataset['name'].lower())\n",
    "    meta_data += preprocessor(dataset['path'],dataset['meta_file_train'])\n",
    "      \n",
    "meta_data= list(meta_data)\n",
    "#random.shuffle(meta_data)\n",
    "\n",
    "meta_data = meta_data\n",
    "embeddings_dict = {}\n",
    "len_meta_data= len(meta_data)\n",
    "for i in tqdm(range(len_meta_data)):\n",
    "    _, wave_file_path, speaker_id = meta_data[i]\n",
    "    try:\n",
    "        sound = audio.from_wav(wave_file_path)\n",
    "    except:\n",
    "        continue\n",
    "    wave = remove_silence(sound)\n",
    "    if wave is None:\n",
    "        continue\n",
    "    \n",
    "    file_embeddings = None\n",
    "    if int(wave.duration_seconds) > 5: # 5 seconds is the Speech2Phone input\n",
    "        begin = 0\n",
    "        end = 5\n",
    "        step = 5\n",
    "        while (end) < int(wave.duration_seconds):\n",
    "            try:        \n",
    "                segment = wave[begin*1000:end*1000]\n",
    "                segment.export('aux' + '.wav', 'wav')# its necessary because pydub and librosa load wave in diferent form \n",
    "                y, sr = librosa.load('aux.wav',sr=22050)#sample rate = 22050 \n",
    "                \n",
    "                if file_embeddings is None:\n",
    "                    file_embeddings =[np.array(encoding_model.predict([librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)])[0])]\n",
    "                else:\n",
    "                    file_embeddings.append(np.array(encoding_model.predict([librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)])[0]))   \n",
    "                os.system('rm aux.wav')\n",
    "                begin = begin + step\n",
    "                end = end + step\n",
    "            except:\n",
    "                print('deu erro')\n",
    "                begin = begin + step\n",
    "                end = end + step\n",
    "                break\n",
    "                \n",
    "      \n",
    "        if speaker_id in embeddings_dict.keys():\n",
    "            embeddings_dict[speaker_id].append(np.mean(np.array(file_embeddings), axis=0) if len(file_embeddings) > 1 else np.array(file_embeddings))\n",
    "            \n",
    "        else:\n",
    "            embeddings_dict[speaker_id]= [np.mean(np.array(file_embeddings), axis=0) if len(file_embeddings) > 1 else np.array(file_embeddings)]\n",
    "\n",
    "        del file_embeddings\n",
    "\n",
    "\n",
    "for speaker_id in embeddings_dict.keys():\n",
    "    embeddings_dict[speaker_id] = np.mean(np.array(embeddings_dict[speaker_id]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and export speakers.json  and aplly a L2_norm in embedding\n",
    "speaker_mapping = {sample: {'name': embeddings_dict[sample][1], 'embedding':torch.nn.functional.normalize(torch.FloatTensor([embeddings_dict[sample][0].reshape(-1).tolist()]), p=2, dim=1).reshape(-1).tolist()} for i, sample in enumerate(embeddings_dict.keys())}\n",
    "save_speaker_mapping(ROOT_PATH, speaker_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test load integrity\n",
    "speaker_mapping_load = load_speaker_mapping(ROOT_PATH)\n",
    "assert speaker_mapping == speaker_mapping_load\n",
    "print(\"The file speakers.json has been exported to \",ROOT_PATH, ' with ', len(embeddings_dict.keys()), ' speakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
